{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trp7TOZhauPY"
      },
      "source": [
        "# Installing and Setting up PySpark and Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TgDAF25SUzM",
        "outputId": "ce860296-3f1b-40c3-8085-71b514f5a64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=fec71519ff718236666304e613745221889a6156a77361f425b6533bed4c9535\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.8)\n",
            "Dataset URL: https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data\n",
            "License(s): CC0-1.0\n",
            "Downloading craigslist-carstrucks-data.zip to /content\n",
            "100% 262M/262M [00:01<00:00, 203MB/s]\n",
            "100% 262M/262M [00:01<00:00, 183MB/s]\n",
            "Archive:  craigslist-carstrucks-data.zip\n",
            "  inflating: vehicles.csv            \n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install pyspark\n",
        "!pip install kaggle\n",
        "\n",
        "# Setup Kaggle API to download dataset\n",
        "!mkdir ~/.kaggle\n",
        "!kaggle datasets download -d austinreese/craigslist-carstrucks-data\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip craigslist-carstrucks-data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSTboCPld0Ib"
      },
      "source": [
        "# Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPit-n0sUJRJ",
        "outputId": "901e229b-813c-4ede-d409-c9e0c01593e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark Reading Time: 35.66996765136719 seconds\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- url: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- region_url: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            " |-- manufacturer: string (nullable = true)\n",
            " |-- model: string (nullable = true)\n",
            " |-- condition: string (nullable = true)\n",
            " |-- cylinders: string (nullable = true)\n",
            " |-- fuel: string (nullable = true)\n",
            " |-- odometer: string (nullable = true)\n",
            " |-- title_status: string (nullable = true)\n",
            " |-- transmission: string (nullable = true)\n",
            " |-- VIN: string (nullable = true)\n",
            " |-- drive: string (nullable = true)\n",
            " |-- size: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- paint_color: string (nullable = true)\n",
            " |-- image_url: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- county: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- lat: string (nullable = true)\n",
            " |-- long: string (nullable = true)\n",
            " |-- posting_date: string (nullable = true)\n",
            "\n",
            "+----------+--------------------+--------------------+--------------------+-----+----+------------+-----+---------+---------+----+--------+------------+------------+----+-----+----+----+-----------+---------+-----------+------+-----+----+----+------------+\n",
            "|        id|                 url|              region|          region_url|price|year|manufacturer|model|condition|cylinders|fuel|odometer|title_status|transmission| VIN|drive|size|type|paint_color|image_url|description|county|state| lat|long|posting_date|\n",
            "+----------+--------------------+--------------------+--------------------+-----+----+------------+-----+---------+---------+----+--------+------------+------------+----+-----+----+----+-----------+---------+-----------+------+-----+----+----+------------+\n",
            "|7222695916|https://prescott....|            prescott|https://prescott....| 6000|NULL|        NULL| NULL|     NULL|     NULL|NULL|    NULL|        NULL|        NULL|NULL| NULL|NULL|NULL|       NULL|     NULL|       NULL|  NULL|   az|NULL|NULL|        NULL|\n",
            "|7218891961|https://fayar.cra...|        fayetteville|https://fayar.cra...|11900|NULL|        NULL| NULL|     NULL|     NULL|NULL|    NULL|        NULL|        NULL|NULL| NULL|NULL|NULL|       NULL|     NULL|       NULL|  NULL|   ar|NULL|NULL|        NULL|\n",
            "|7221797935|https://keys.crai...|        florida keys|https://keys.crai...|21000|NULL|        NULL| NULL|     NULL|     NULL|NULL|    NULL|        NULL|        NULL|NULL| NULL|NULL|NULL|       NULL|     NULL|       NULL|  NULL|   fl|NULL|NULL|        NULL|\n",
            "|7222270760|https://worcester...|worcester / centr...|https://worcester...| 1500|NULL|        NULL| NULL|     NULL|     NULL|NULL|    NULL|        NULL|        NULL|NULL| NULL|NULL|NULL|       NULL|     NULL|       NULL|  NULL|   ma|NULL|NULL|        NULL|\n",
            "|7210384030|https://greensbor...|          greensboro|https://greensbor...| 4900|NULL|        NULL| NULL|     NULL|     NULL|NULL|    NULL|        NULL|        NULL|NULL| NULL|NULL|NULL|       NULL|     NULL|       NULL|  NULL|   nc|NULL|NULL|        NULL|\n",
            "+----------+--------------------+--------------------+--------------------+-----+----+------------+-----+---------+---------+----+--------+------------+------------+----+-----+----+----+-----------+---------+-----------+------+-----+----+----+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import time\n",
        "from pyspark.sql.functions import col, lit\n",
        "\n",
        "# Create SparkSession (required to work with PySpark)\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Start timer and read the CSV file into a PySpark DataFrame\n",
        "start_time = time.time()\n",
        "df_spark = spark.read.csv(\"/content/vehicles.csv\", header=True, inferSchema=True)\n",
        "end_time = time.time()\n",
        "\n",
        "# Display time taken to read the file\n",
        "print(f\"PySpark Reading Time: {end_time - start_time} seconds\")\n",
        "\n",
        "# Display schema of the DataFrame\n",
        "df_spark.printSchema()\n",
        "\n",
        "# Show the first 5 rows of the dataset\n",
        "df_spark.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIz-axk4gIVr"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VTWYY-9XjIoU"
      },
      "outputs": [],
      "source": [
        "# Drop columns that are not useful for our analysis or model\n",
        "columns_to_drop = ['url', 'region', 'region_url', 'title_status', 'VIN', 'size', 'image_url', 'lat', 'long', 'county', 'description']\n",
        "df_spark = df_spark.drop(*columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY1BMpMWULHw",
        "outputId": "88d3a926-b3d9-410c-d39a-2de38feab00c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- manufacturer: string (nullable = true)\n",
            " |-- model: string (nullable = true)\n",
            " |-- condition: string (nullable = true)\n",
            " |-- cylinders: string (nullable = true)\n",
            " |-- fuel: string (nullable = true)\n",
            " |-- odometer: double (nullable = true)\n",
            " |-- transmission: string (nullable = true)\n",
            " |-- drive: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- paint_color: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- posting_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert columns to appropriate data types for analysis and modeling\n",
        "df_spark = df_spark.withColumn(\"price\", col(\"price\").cast(\"double\"))  # Price should be numeric\n",
        "df_spark = df_spark.withColumn(\"year\", col(\"year\").cast(\"int\"))  # Year should be an integer\n",
        "df_spark = df_spark.withColumn(\"odometer\", col(\"odometer\").cast(\"double\"))  # Odometer should be numeric\n",
        "\n",
        "# Print the updated schema to verify data types\n",
        "df_spark.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4_OSdp5TcPPW"
      },
      "outputs": [],
      "source": [
        "# Fill missing categorical values with 'unknown' for consistency\n",
        "df_spark = df_spark.fillna({\n",
        "    \"cylinders\": \"unknown\",\n",
        "    \"fuel\": \"unknown\",\n",
        "    \"transmission\": \"unknown\",\n",
        "    \"drive\": \"unknown\",\n",
        "    \"paint_color\": \"unknown\",\n",
        "    \"type\": \"unknown\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RMB0jt6mdGL",
        "outputId": "9145e0d3-a5c1-4195-f4a0-53be70cbb896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|year|\n",
            "+----+\n",
            "|1901|\n",
            "|1902|\n",
            "|1903|\n",
            "|1905|\n",
            "|1909|\n",
            "|1910|\n",
            "|1913|\n",
            "|1915|\n",
            "|1916|\n",
            "|1918|\n",
            "|1920|\n",
            "|1921|\n",
            "|1922|\n",
            "|1923|\n",
            "|1924|\n",
            "|1925|\n",
            "|1926|\n",
            "|1927|\n",
            "|1928|\n",
            "|1929|\n",
            "+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter record with invalid year\n",
        "df_spark = df_spark.filter((col(\"year\") > 1900) & (col(\"year\") <= 2024))\n",
        "\n",
        "df_spark.select(\"year\").distinct().orderBy(\"year\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urfA12sCcia1",
        "outputId": "3006723e-3775-4664-e37a-22ce9122c483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|year|age|\n",
            "+----+---+\n",
            "|2014| 10|\n",
            "|2010| 14|\n",
            "|2020|  4|\n",
            "|2017|  7|\n",
            "|2013| 11|\n",
            "+----+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Calculate age based on the current year\n",
        "current_year = 2024\n",
        "df_spark = df_spark.withColumn(\"age\", lit(current_year) - col(\"year\"))\n",
        "\n",
        "# Show result\n",
        "df_spark.select(\"year\", \"age\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XL2rs26eMtM"
      },
      "source": [
        "# EDA (Exploratory Data Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrFdPfRvjiar",
        "outputId": "f77e2a93-b705-4364-916e-c42d852a0e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|year|count|\n",
            "+----+-----+\n",
            "|1901|    3|\n",
            "|1902|    1|\n",
            "|1903|   12|\n",
            "|1905|    1|\n",
            "|1909|    1|\n",
            "|1910|    2|\n",
            "|1913|    2|\n",
            "|1915|    1|\n",
            "|1916|    2|\n",
            "|1918|    1|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Number of vehicles per year\n",
        "df_spark.groupBy(\"year\").count().orderBy(\"year\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rYKVcWojltY",
        "outputId": "fb18ebac-3b98-4047-aa61-7e53cb21f21e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+------------------+-----------------+\n",
            "|summary|               price|          odometer|              age|\n",
            "+-------+--------------------+------------------+-----------------+\n",
            "|  count|              425827|            421332|           425929|\n",
            "|   mean|   75279.43337552574| 98224.43318807971|12.76114563694888|\n",
            "| stddev|1.2197335159309383E7|214118.86783963133|9.431055628948746|\n",
            "|    min|                 0.0|               0.0|                2|\n",
            "|    max|       3.736928711E9|             1.0E7|              123|\n",
            "+-------+--------------------+------------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Descriptive statistics for numeric columns\n",
        "df_spark.describe([\"price\", \"odometer\", \"age\"]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJHnQJ0Ll3dH",
        "outputId": "ae385794-4119-40cf-b830-e27968492c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+------------+--------------------+\n",
            "|year|  price|manufacturer|               model|\n",
            "+----+-------+------------+--------------------+\n",
            "|2014|33590.0|         gmc|sierra 1500 crew ...|\n",
            "|2010|22590.0|   chevrolet|      silverado 1500|\n",
            "|2020|39590.0|   chevrolet| silverado 1500 crew|\n",
            "|2017|30990.0|      toyota|tundra double cab sr|\n",
            "|2013|15000.0|        ford|           f-150 xlt|\n",
            "+----+-------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter cars priced under $100,000 and manufactured 1990 or later\n",
        "df_filtered = df_spark.filter((col(\"price\") < 100000) & (col(\"year\") >= 1990))\n",
        "df_filtered.select(\"year\", \"price\", \"manufacturer\", \"model\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbjFv_xOot-B",
        "outputId": "70c7970d-4940-4db3-bc89-5b8426264f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+------------+--------------------+\n",
            "|year|  price|manufacturer|               model|\n",
            "+----+-------+------------+--------------------+\n",
            "|2014|33590.0|         gmc|sierra 1500 crew ...|\n",
            "|2010|22590.0|   chevrolet|      silverado 1500|\n",
            "|2020|39590.0|   chevrolet| silverado 1500 crew|\n",
            "|2017|30990.0|      toyota|tundra double cab sr|\n",
            "|2013|15000.0|        ford|           f-150 xlt|\n",
            "+----+-------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use PySpark SQL\n",
        "df_spark.createOrReplaceTempView(\"vehicles\")\n",
        "spark.sql(\"SELECT year, price, manufacturer, model FROM vehicles WHERE price < 100000 AND year >= 1990\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v58WMwtIjmN7",
        "outputId": "ba2d203d-5034-467b-f035-d022567a7123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+-------+\n",
            "|year|odometer|  price|\n",
            "+----+--------+-------+\n",
            "|2013|128000.0|15000.0|\n",
            "|1992|192000.0| 4500.0|\n",
            "|2001|144700.0|22500.0|\n",
            "|2004|176144.0| 3000.0|\n",
            "|2008|201300.0|17500.0|\n",
            "+----+--------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter car odometer > 100,000\n",
        "df_filtered = df_spark.filter(col(\"odometer\") > 100000)\n",
        "df_filtered.select(\"year\", \"odometer\", \"price\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkUO8V6AoyVU",
        "outputId": "a4ccee1d-70a8-4f5f-834e-48fb6f7b56f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+-------+\n",
            "|year|odometer|  price|\n",
            "+----+--------+-------+\n",
            "|2013|128000.0|15000.0|\n",
            "|1992|192000.0| 4500.0|\n",
            "|2001|144700.0|22500.0|\n",
            "|2004|176144.0| 3000.0|\n",
            "|2008|201300.0|17500.0|\n",
            "+----+--------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use PySpark SQL\n",
        "spark.sql(\"SELECT year, odometer, price FROM vehicles WHERE odometer > 100000\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WT3RgKYo57w"
      },
      "source": [
        "## Comparing to PySpark and Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7A7aulc6HJ5"
      },
      "source": [
        "## PySpark version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfksoemmjnzo",
        "outputId": "5f80a281-8d1e-4bec-96c3-4968c787df1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark Reading Time: 18.788039207458496 seconds\n",
            "PySpark Processing Time: 0.2909984588623047 seconds\n"
          ]
        }
      ],
      "source": [
        "# Read data again\n",
        "start_time = time.time()\n",
        "df_spark = spark.read.csv(\"/content/vehicles.csv\", header=True, inferSchema=True)\n",
        "end_time = time.time()\n",
        "\n",
        "# Display time taken to read the file\n",
        "print(f\"PySpark Reading Time: {end_time - start_time} seconds\")\n",
        "\n",
        "# Set the current year for age calculation\n",
        "current_year = 2024\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Drop unused columns\n",
        "columns_to_drop =['url', 'region', 'region_url', 'title_status', 'VIN', 'size', 'image_url', 'lat', 'long', 'county', 'description']\n",
        "df_spark = df_spark.drop(*columns_to_drop)\n",
        "\n",
        "# Calculate age based on the current year\n",
        "df_spark = df_spark.withColumn(\"age\", lit(current_year) - col(\"year\"))\n",
        "\n",
        "# Convert columns to appropriate data types\n",
        "df_spark = df_spark.withColumn(\"price\", col(\"price\").cast(\"double\"))\n",
        "df_spark = df_spark.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
        "df_spark = df_spark.withColumn(\"odometer\", col(\"odometer\").cast(\"double\"))\n",
        "\n",
        "# Filter record with invalid year\n",
        "df_spark = df_spark.filter((col(\"year\") > 1900) & (col(\"year\") <= 2024))\n",
        "\n",
        "# Fill missing values with default values\n",
        "df_spark = df_spark.fillna({\n",
        "    \"cylinders\": \"unknown\",\n",
        "    \"fuel\": \"unknown\",\n",
        "    \"transmission\": \"unknown\",\n",
        "    \"drive\": \"unknown\",\n",
        "    \"paint_color\": \"unknown\",\n",
        "    \"type\": \"unknown\"\n",
        "})\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"PySpark Processing Time: {end_time - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_fiFB_U6CwP"
      },
      "source": [
        "## Pandas version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a86EBFey5wM6",
        "outputId": "7b13d863-fc8a-4241-d4b1-45b7e41d6e21"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pandas Reading Time: 33.5674831867218 seconds\n",
            "Pandas Processing Time: 2.1332733631134033 seconds\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "current_year = 2024\n",
        "\n",
        "# Start timer for reading the data\n",
        "start_time_read = time.time()\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df_pandas = pd.read_csv(\"/content/vehicles.csv\")\n",
        "\n",
        "end_time_read = time.time()\n",
        "print(f\"Pandas Reading Time: {end_time_read - start_time_read} seconds\")\n",
        "\n",
        "# Start timer for data processing\n",
        "start_time_process = time.time()\n",
        "\n",
        "# Drop unused columns\n",
        "columns_to_drop = ['url', 'region', 'region_url', 'title_status', 'VIN', 'size', 'image_url', 'lat', 'long', 'county', 'description']\n",
        "df_pandas = df_pandas.drop(columns=columns_to_drop)\n",
        "\n",
        "# Calculate age based on the current year\n",
        "df_pandas['age'] = current_year - df_pandas['year']\n",
        "\n",
        "# Convert columns to appropriate data types\n",
        "df_pandas['price'] = pd.to_numeric(df_pandas['price'], errors='coerce')\n",
        "df_pandas['year'] = pd.to_numeric(df_pandas['year'], errors='coerce')\n",
        "df_pandas['odometer'] = pd.to_numeric(df_pandas['odometer'], errors='coerce')\n",
        "\n",
        "# Filter records with invalid year\n",
        "df_pandas = df_pandas[(df_pandas['year'] > 1900) & (df_pandas['year'] <= 2024)]\n",
        "\n",
        "# Fill missing values with default values\n",
        "df_pandas.loc['cylinders']=df_pandas['cylinders'].fillna('unknown')\n",
        "df_pandas.loc['fuel']=df_pandas['fuel'].fillna('unknown')\n",
        "df_pandas.loc['transmission']=df_pandas['transmission'].fillna('unknown')\n",
        "df_pandas.loc['drive']=df_pandas['drive'].fillna('unknown')\n",
        "df_pandas.loc['paint_color']=df_pandas['paint_color'].fillna('unknown')\n",
        "df_pandas.loc['type']=df_pandas['type'].fillna('unknown')\n",
        "\n",
        "# End timer for data processing\n",
        "end_time_process = time.time()\n",
        "\n",
        "# Print processing time\n",
        "print(f\"Pandas Processing Time: {end_time_process - start_time_process} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "looab2QZmEty"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA1dXnWahMj-"
      },
      "source": [
        "## Step 1: Data Cleaning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocURbXja6gbN"
      },
      "source": [
        "### 1.1 Drop rows with missing values in important columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xpcEtjeAhuJw"
      },
      "outputs": [],
      "source": [
        "# We remove rows that have missing values in the 'price', 'odometer', 'year', or 'manufacturer' columns.\n",
        "df_spark = df_spark.dropna(subset=['price', 'odometer', 'year', 'manufacturer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIjLguA-2D4c"
      },
      "source": [
        "### 1.2 Drop outliers using Z-score for numeric columns\n",
        "Calculate Z-scores and filter out records where Z-score > 2 (approximately 95% confidence interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mhcczUGJ2EYb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import mean, stddev\n",
        "\n",
        "# Calculate mean and standard deviation for the numeric columns\n",
        "mean_price, stddev_price = df_spark.select(mean('price'), stddev('price')).first()\n",
        "mean_odometer, stddev_odometer = df_spark.select(mean('odometer'), stddev('odometer')).first()\n",
        "\n",
        "# Filter out outliers based on Z-score > 2\n",
        "df_spark = df_spark.filter(((df_spark['price'] - mean_price) / stddev_price).between(-2, 2))\n",
        "df_spark = df_spark.filter(((df_spark['odometer'] - mean_odometer) / stddev_odometer).between(-2, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-973rlw1sYk"
      },
      "source": [
        "## Step 2: Feature Engineering\n",
        "Convert categorical columns to numeric using StringIndexer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KELV2Z9I1rsM"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Convert 'manufacturer' to numeric\n",
        "indexer_manufacturer = StringIndexer(inputCol=\"manufacturer\", outputCol=\"manufacturer_index\")\n",
        "df_spark = indexer_manufacturer.fit(df_spark).transform(df_spark)\n",
        "\n",
        "# Convert 'fuel' to numeric\n",
        "indexer_fuel = StringIndexer(inputCol=\"fuel\", outputCol=\"fuel_index\")\n",
        "df_spark = indexer_fuel.fit(df_spark).transform(df_spark)\n",
        "\n",
        "# Convert 'transmission' to numeric\n",
        "indexer_transmission = StringIndexer(inputCol=\"transmission\", outputCol=\"transmission_index\")\n",
        "df_spark = indexer_transmission.fit(df_spark).transform(df_spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He7K1LsV1yz0"
      },
      "source": [
        "## Step 3: Prepare Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJDvWpyP6dri"
      },
      "source": [
        "### 3.1 Assemble features into a single vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Yv3Xv7s-1xjo"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# List of features to be used in the model\n",
        "features = ['year', 'odometer', 'manufacturer_index', 'fuel_index', 'transmission_index']\n",
        "\n",
        "# Assemble these features into a single feature vector\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "df_with_features = assembler.transform(df_spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dBnRYFi2oKa"
      },
      "source": [
        "### 3.2 Normalize the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_d2gyo-z2nZP"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "# Apply standard scaling to normalize the feature vector (zero mean, unit variance)\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
        "scaler_model = scaler.fit(df_with_features)\n",
        "df_with_scaled_features = scaler_model.transform(df_with_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooqP4h5s2uxi"
      },
      "source": [
        "## Step 4: Train-Test Split\n",
        "\n",
        "### Split data into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uzZoN2ci2xXp"
      },
      "outputs": [],
      "source": [
        "# We use an 80-20 split for training and testing the model.\n",
        "train_data, test_data = df_with_scaled_features.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ikrDiWi23KS"
      },
      "source": [
        "## Step 5: Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0moalZEl25Rk"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "\n",
        "# Initialize the Decision Tree model\n",
        "dt = DecisionTreeRegressor(featuresCol=\"scaled_features\", labelCol=\"price\")\n",
        "\n",
        "# Train the model on the training data\n",
        "model = dt.fit(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpoVjXKT3AIL"
      },
      "source": [
        "## Step 6: Model Prediction & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkM3qP0527Uk",
        "outputId": "e72f376d-a096-40a5-c324-ccce14476527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 3224698814.7563186\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test data\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model using Mean Squared Error (MSE)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Initialize the evaluator with MSE as the metric\n",
        "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "\n",
        "# Calculate MSE on the test data\n",
        "mse = evaluator.evaluate(predictions)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some predictions and true price\n",
        "predictions.select(\"prediction\", \"price\").show(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0KD3kDTB4Yo",
        "outputId": "0e634323-3806-488c-c42e-75e7322d090a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------+\n",
            "|        prediction|  price|\n",
            "+------------------+-------+\n",
            "|28139.301985584116|29590.0|\n",
            "|32122.851198990324|22990.0|\n",
            "|15693.980246567666|22590.0|\n",
            "| 27222.18423206329|38990.0|\n",
            "|15766.982151244716|30990.0|\n",
            "|32122.851198990324|21990.0|\n",
            "|32122.851198990324|38990.0|\n",
            "|12823.033226081818|27990.0|\n",
            "|14503.288756167229|18500.0|\n",
            "|  6743.16269446042|12977.0|\n",
            "|15693.980246567666|20977.0|\n",
            "|19544.021471873202|40590.0|\n",
            "|32122.851198990324|22488.0|\n",
            "| 26951.87902542373|19995.0|\n",
            "|28139.301985584116|30990.0|\n",
            "|  6743.16269446042| 5000.0|\n",
            "|23136.404829488667|27590.0|\n",
            "| 27222.18423206329|32590.0|\n",
            "|28139.301985584116|30990.0|\n",
            "|28139.301985584116|30990.0|\n",
            "+------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}